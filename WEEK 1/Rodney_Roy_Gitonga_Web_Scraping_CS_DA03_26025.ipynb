{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O7JRdT2WXpz8",
        "outputId": "068ec141-b4b2-4072-a41a-353467b87037"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Successfully connected to the website!\n",
            "Columns found: ['Team Name', 'Year', 'Wins', 'Losses', 'OT Losses', 'Win %', 'Goals For (GF)', 'Goals Against (GA)', '+ / -']\n",
            "\n",
            "First 5 rows of the dataset:\n",
            "            Team Name  Year Wins Losses OT Losses  Win % Goals For (GF)  \\\n",
            "0       Boston Bruins  1990   44     24             0.55            299   \n",
            "1      Buffalo Sabres  1990   31     30            0.388            292   \n",
            "2      Calgary Flames  1990   46     26            0.575            344   \n",
            "3  Chicago Blackhawks  1990   49     23            0.613            284   \n",
            "4   Detroit Red Wings  1990   34     38            0.425            273   \n",
            "\n",
            "  Goals Against (GA) + / -  \n",
            "0                264    35  \n",
            "1                278    14  \n",
            "2                263    81  \n",
            "3                211    73  \n",
            "4                298   -25  \n",
            "\n",
            "Data successfully saved to Hockey_Data_Scraped.csv\n"
          ]
        }
      ],
      "source": [
        "'''\n",
        "  Title: Web Scraping Project - Hockey Teams\n",
        "  Name: Rodney Roy Gitonga\n",
        "  Cybershujaa id: CS-DA26025\n",
        "  Date: 15 January 2026\n",
        "  Description: This script scrapes hockey team data from scrapethissite.com,\n",
        "  parses the HTML using BeautifulSoup, organizes the data into a Pandas DataFrame,\n",
        "  and exports it to a CSV file.\n",
        "'''\n",
        "\n",
        "# Import libraries\n",
        "from bs4 import BeautifulSoup\n",
        "import requests\n",
        "import pandas as pd\n",
        "\n",
        "# Send HTTP Request\n",
        "# Define the target URL and use requests to fetch the page content.\n",
        "url = 'https://www.scrapethissite.com/pages/forms/'\n",
        "response = requests.get(url)\n",
        "\n",
        "# Check request was successful (Status Code 200)\n",
        "if response.status_code == 200:\n",
        "    print(\"Successfully connected to the website!\")\n",
        "else:\n",
        "    print(f\"Failed to connect. Status code: {response.status_code}\")\n",
        "\n",
        "# Parse HTML Content\n",
        "# Initialize BeautifulSoup to parse the text of the response\n",
        "soup = BeautifulSoup(response.text, 'html.parser')\n",
        "\n",
        "# Locate Target Table\n",
        "# We look for the first table with the class 'table'\n",
        "hockey_table = soup.find('table', class_='table')\n",
        "\n",
        "# Extract Column Headers\n",
        "# Find all table header cells ('th') and strip whitespace\n",
        "header_tags = hockey_table.find_all('th')\n",
        "columns = [header.text.strip() for header in header_tags]\n",
        "print(f\"Columns found: {columns}\")\n",
        "\n",
        "# Extract Row Data\n",
        "# Find all table rows ('tr'). Skip the first row [1:] because it contains headers.\n",
        "rows = hockey_table.find_all('tr')\n",
        "extracted_data = []\n",
        "\n",
        "for row in rows[1:]:\n",
        "    # Find all data cells ('td') in the current row\n",
        "    cells = row.find_all('td')\n",
        "    # Clean the text for each cell\n",
        "    row_data = [cell.text.strip() for cell in cells]\n",
        "    # Append the clean row to our list\n",
        "    extracted_data.append(row_data)\n",
        "\n",
        "# Create DataFrame\n",
        "# Convert the list of lists into a Pandas DataFrame\n",
        "df = pd.DataFrame(extracted_data, columns=columns)\n",
        "\n",
        "# Data Inspection\n",
        "# Display the first 5 rows to verify data integrity\n",
        "print(\"\\nFirst 5 rows of the dataset:\")\n",
        "print(df.head())\n",
        "\n",
        "# Export to CSV\n",
        "# Save the file without the pandas index column\n",
        "csv_filename = 'Hockey_Data_Scraped.csv'\n",
        "df.to_csv(csv_filename, index=False)\n",
        "print(f\"\\nData successfully saved to {csv_filename}\")"
      ]
    }
  ]
}