{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.12.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":2654038,"sourceType":"datasetVersion","datasetId":434238}],"dockerImageVersionId":31259,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"\"\"\"\nTitle : Netflix Data Wrangling Project\nName: Rodney Roy Gitonga\nCybershujaa ID :CS-DA03-26025\nProgram:DA1-2026\nDate:21/01/2026\n\nDescription: This project focuses on cleaning, structuring and validating the Netflix Movies and TV shows Dataset\n\n\n\"\"\"\nimport pandas as pd\nimport numpy as np\nimport datetime as dt\n\n# 1. LOAD DATA\n# Load the dataset from the specific Kaggle directory\ndf = pd.read_csv('/kaggle/input/netflix-shows/netflix_titles.csv')\n\n\n# 2. DISCOVERY\n\nprint(\"--- DISCOVERY ---\")\nprint(\"Shape of the dataset:\", df.shape)\nprint(\"\\nMissing values per column:\\n\", df.isnull().sum())\nprint(\"\\nDuplicate rows:\", df.duplicated().sum())\n\n\n# 3. STRUCTURING\n\nprint(\"\\n--- STRUCTURING ---\")\n\n# Convert 'date_added' to datetime format\n# errors='coerce' turns invalid dates into NaT (Not a Time) to prevent crashes\ndf['date_added'] = pd.to_datetime(df['date_added'].str.strip(), format='mixed', errors='coerce')\n\n# Separate 'duration' into value and unit\n# We use regex to find the numbers (\\d+) and the text (\\w+)\ndf[['duration_value', 'duration_unit']] = df['duration'].str.extract(r'(\\d+)\\s*(\\w+)')\n\n# Convert duration_value to numeric\ndf['duration_value'] = pd.to_numeric(df['duration_value'])\n\nprint(\"Duration columns created successfully.\")\n\n\n# 4. CLEANING\n\nprint(\"\\n--- CLEANING ---\")\n\n# Drop exact duplicates\ndf = df.drop_duplicates()\n\n# Drop the 'description' column as per instructions\ndf = df.drop(columns=['description'])\n\n# --- Impute Missing Directors (Optimized) ---\n# Logic: If a Director+Cast combo appears 3+ times, use that to fill missing directors.\n# Create a temporary column for the pair\ndf['dir_cast'] = df['director'] + '---' + df['cast']\ncounts = df['dir_cast'].value_counts()\n# Filter for pairs that appear 3 or more times\nfrequent_pairs = counts[counts >= 3].index\n\n# Create a dictionary for mapping: {Cast_String : Director_Name}\ndirector_map = {}\nfor pair in frequent_pairs:\n    if isinstance(pair, str):\n        parts = pair.split('---')\n        if len(parts) == 2:\n            director_map[parts[1]] = parts[0]\n\n# Fill missing directors using the map\ndf['director'] = df['director'].fillna(df['cast'].map(director_map))\n# Fill remaining missing directors with 'Not Given'\ndf['director'] = df['director'].fillna('Not Given')\n\n\n# --- Impute Missing Countries (Optimized) ---\n# Logic: Use the Director to find the Country\n# Create a mapping dictionary: {Director_Name : Country_Name}\n# We drop rows where director or country is NaN to build a clean reference map\nclean_subset = df.dropna(subset=['director', 'country'])\ncountry_map = dict(zip(clean_subset['director'], clean_subset['country']))\n\n# Fill missing countries using the director map\ndf['country'] = df['country'].fillna(df['director'].map(country_map))\n# Fill remaining missing countries with 'Not Given'\ndf['country'] = df['country'].fillna('Not Given')\n\n# Fill missing cast with 'Not Given'\ndf['cast'] = df['cast'].fillna('Not Given')\n\n# Drop rows where specific crucial fields are still null\ndf.dropna(subset=['date_added', 'rating', 'duration'], inplace=True)\n\nprint(\"Missing values after cleaning:\\n\", df.isnull().sum())\n\n\n# 5. ERROR HANDLING & VALIDATION\n\nprint(\"\\n--- VALIDATION ---\")\n\n# Check for logical error: Date Added cannot be before Release Year\n# We extract the year from date_added\ndf['added_year'] = df['date_added'].dt.year\n\n# Count inconsistent records\ninconsistent_dates = df[df['added_year'] < df['release_year']]\nprint(f\"Records where date_added < release_year: {len(inconsistent_dates)}\")\n\n# (Optional) Fix or Drop inconsistent dates.\n# For this assignment, we will drop them to ensure data logical accuracy.\ndf = df.drop(inconsistent_dates.index)\n\n# Remove temporary columns used for wrangling\ncols_to_drop = ['dir_cast', 'added_year']\n# Only drop if they exist\ndf.drop(columns=[c for c in cols_to_drop if c in df.columns], inplace=True)\n\n# Final check\nprint(\"\\nFinal shape:\", df.shape)\nprint(\"Final Missing values:\\n\", df.isnull().sum())\n\n\n# 6. EXPORT / PUBLISH\n\n# Save to the working directory in Kaggle\ndf.to_csv('cleaned_netflix.csv', index=False)\nprint(\"\\nFile 'cleaned_netflix.csv' saved successfully!\")","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2026-01-20T13:02:15.054123Z","iopub.execute_input":"2026-01-20T13:02:15.054810Z","iopub.status.idle":"2026-01-20T13:02:15.609913Z","shell.execute_reply.started":"2026-01-20T13:02:15.054777Z","shell.execute_reply":"2026-01-20T13:02:15.609171Z"}},"outputs":[],"execution_count":null}]}